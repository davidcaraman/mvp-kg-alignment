{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the project called Entity Alignment using Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the script for the dataset processing. We are going to map the entity ids to the entity name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (4500, 3)\n",
      "\n",
      "First few rows:\n",
      "   index                                         candidates  num_candidates\n",
      "0      0  [10500, 33775, 36175, 17181, 20105, 32176, 117...              20\n",
      "1      1  [10501, 36449, 11420, 18572, 33228, 15363, 333...              20\n",
      "2      2  [20759, 10502, 18084, 16541, 13414, 15062, 332...              20\n",
      "3      3  [10503, 15732, 31027, 19403, 38929, 37146, 176...              20\n",
      "4      4  [33507, 15190, 10504, 19425, 38842, 37861, 324...              20\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4500 entries, 0 to 4499\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   index           4500 non-null   int64 \n",
      " 1   candidates      4500 non-null   object\n",
      " 2   num_candidates  4500 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 105.6+ KB\n",
      "None\n",
      "\n",
      "Sample candidates for first entity:\n",
      "Entity 0: [10500, 33775, 36175, 17181, 20105, 32176, 11787, 38085, 37257, 33904]...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train.cand_list.20 file as a pandas DataFrame\n",
    "file_path = \"data/DBP15K/torch_geometric_cache/raw/fr_en/train.cand_list.20\"\n",
    "\n",
    "# Read the file and parse it\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # Split by ': ' to separate index from candidate list\n",
    "            parts = line.split(': ', 1)\n",
    "            if len(parts) == 2:\n",
    "                index = int(parts[0])\n",
    "                candidates = [int(x) for x in parts[1].split()]\n",
    "                data.append({'index': index, 'candidates': candidates, 'num_candidates': len(candidates)})\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nSample candidates for first entity:\")\n",
    "print(f\"Entity {df.iloc[0]['index']}: {df.iloc[0]['candidates'][:10]}...\")  # Show first 10 candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>candidates</th>\n",
       "      <th>num_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[10500, 33775, 36175, 17181, 20105, 32176, 117...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[10501, 36449, 11420, 18572, 33228, 15363, 333...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[20759, 10502, 18084, 16541, 13414, 15062, 332...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[10503, 15732, 31027, 19403, 38929, 37146, 176...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[33507, 15190, 10504, 19425, 38842, 37861, 324...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         candidates  num_candidates\n",
       "0      0  [10500, 33775, 36175, 17181, 20105, 32176, 117...              20\n",
       "1      1  [10501, 36449, 11420, 18572, 33228, 15363, 333...              20\n",
       "2      2  [20759, 10502, 18084, 16541, 13414, 15062, 332...              20\n",
       "3      3  [10503, 15732, 31027, 19403, 38929, 37146, 176...              20\n",
       "4      4  [33507, 15190, 10504, 19425, 38842, 37861, 324...              20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have successfully processed the DBP15K entity alignment dataset to create enhanced dataframes with comprehensive entity information including names, URIs, IDs, and language labels. This preprocessing step is crucial for the \"Entity Alignment using Agentic AI\" research project.\n",
    "Key Accomplishments:\n",
    "Data Loading & Parsing:\n",
    "Loaded the train.cand_list.20 file containing 4,500 French entities with 20 English candidate matches each\n",
    "Parsed entity candidate lists with proper data type handling\n",
    "Entity Mapping Integration:\n",
    "Successfully loaded 19,661 French entities from ent_ids_1\n",
    "Successfully loaded 19,993 English entities from ent_ids_2\n",
    "Created comprehensive entity mappings with names extracted from DBpedia URIs\n",
    "Enhanced Dataset Creation:\n",
    "Format: [entity_name, entity_uri, entity_id, language] for both index and candidate entities\n",
    "Index entities: French DBpedia entities (KG1)\n",
    "Candidate entities: English DBpedia entities (KG2)\n",
    "Zero unknown entities - 100% mapping success rate\n",
    "Output Files Generated:\n",
    "entity_alignment_with_names_and_uris.csv: Main dataset with candidate lists as strings (4,500 rows)\n",
    "entity_alignment_flattened_with_uris.csv: Flattened format with each index-candidate pair as separate row (90,000 rows)\n",
    "Data Quality:\n",
    "Total entities processed: 4,500 French index entities\n",
    "Total candidate pairs: 90,000 (exactly 20 candidates per entity)\n",
    "Coverage: 100% entity name resolution (no unknown entities)\n",
    "Languages: Proper French/English language tagging\n",
    "Technical Implementation:\n",
    "Created reusable functions for entity mapping loading\n",
    "Implemented robust error handling for unknown entities\n",
    "Generated comprehensive statistics and validation reports\n",
    "Organized outputs in dedicated processed_data/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entity mappings...\n",
      "Loaded 19661 entities from KG1 (French)\n",
      "Loaded 19993 entities from KG2 (English)\n",
      "\n",
      "Sample mappings from KG1:\n",
      "  ID 0: Saint-Joseph-de-Coleraine (http://fr.dbpedia.org/resource/Saint-Joseph-de-Coleraine)\n",
      "  ID 1: Self_Portrait (http://fr.dbpedia.org/resource/Self_Portrait)\n",
      "  ID 2: Alliance_des_libéraux_et_des_démocrates_pour_l'Europe (http://fr.dbpedia.org/resource/Alliance_des_libéraux_et_des_démocrates_pour_l'Europe)\n",
      "\n",
      "Sample mappings from KG2:\n",
      "  ID 10500: Saint-Joseph-de-Coleraine,_Quebec (http://dbpedia.org/resource/Saint-Joseph-de-Coleraine,_Quebec)\n",
      "  ID 10501: Self_Portrait_(Bob_Dylan_album) (http://dbpedia.org/resource/Self_Portrait_(Bob_Dylan_album))\n",
      "  ID 10502: Alliance_of_Liberals_and_Democrats_for_Europe_Party (http://dbpedia.org/resource/Alliance_of_Liberals_and_Democrats_for_Europe_Party)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Create processed_data directory if it doesn't exist\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "def load_entity_mappings(file_path):\n",
    "    \"\"\"Load entity ID to URI mappings from file\"\"\"\n",
    "    mappings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split('\\t', 1)\n",
    "                if len(parts) == 2:\n",
    "                    entity_id = int(parts[0])\n",
    "                    entity_uri = parts[1]\n",
    "                    # Extract entity name from URI (last part after /)\n",
    "                    entity_name = entity_uri.split('/')[-1]\n",
    "                    mappings[entity_id] = {\n",
    "                        'name': entity_name,\n",
    "                        'uri': entity_uri\n",
    "                    }\n",
    "    return mappings\n",
    "\n",
    "# Load entity mappings\n",
    "print(\"Loading entity mappings...\")\n",
    "kg1_mappings = load_entity_mappings(\"data/DBP15K/torch_geometric_cache/raw/fr_en/ent_ids_1\")  # French entities\n",
    "kg2_mappings = load_entity_mappings(\"data/DBP15K/torch_geometric_cache/raw/fr_en/ent_ids_2\")  # English entities\n",
    "\n",
    "print(f\"Loaded {len(kg1_mappings)} entities from KG1 (French)\")\n",
    "print(f\"Loaded {len(kg2_mappings)} entities from KG2 (English)\")\n",
    "\n",
    "# Check first few mappings\n",
    "print(\"\\nSample mappings from KG1:\")\n",
    "for i, (k, v) in enumerate(list(kg1_mappings.items())[:3]):\n",
    "    print(f\"  ID {k}: {v['name']} ({v['uri']})\")\n",
    "\n",
    "print(\"\\nSample mappings from KG2:\")\n",
    "for i, (k, v) in enumerate(list(kg2_mappings.items())[:3]):\n",
    "    print(f\"  ID {k}: {v['name']} ({v['uri']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced dataframe with entity names and URIs...\n",
      "Enhanced DataFrame shape: (4500, 6)\n",
      "\n",
      "First few rows of enhanced DataFrame:\n",
      "                                          index_name  \\\n",
      "0                          Saint-Joseph-de-Coleraine   \n",
      "1                                      Self_Portrait   \n",
      "2  Alliance_des_libéraux_et_des_démocrates_pour_l...   \n",
      "3                                             Wallon   \n",
      "4                                            Android   \n",
      "\n",
      "                                           index_uri  index_id index_language  \\\n",
      "0  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "1       http://fr.dbpedia.org/resource/Self_Portrait         1         French   \n",
      "2  http://fr.dbpedia.org/resource/Alliance_des_li...         2         French   \n",
      "3              http://fr.dbpedia.org/resource/Wallon         3         French   \n",
      "4             http://fr.dbpedia.org/resource/Android         4         French   \n",
      "\n",
      "   num_candidates  \n",
      "0              20  \n",
      "1              20  \n",
      "2              20  \n",
      "3              20  \n",
      "4              20  \n",
      "\n",
      "Sample candidates for first entity ('Saint-Joseph-de-Coleraine'):\n",
      "  1. Saint-Joseph-de-Coleraine,_Quebec (URI: http://dbpedia.org/resource/Saint-Joseph-de-Coleraine,_Quebec, ID: 10500, Language: English)\n",
      "  2. Saint-Joseph-de-Beauce (URI: http://dbpedia.org/resource/Saint-Joseph-de-Beauce, ID: 33775, Language: English)\n",
      "  3. Saint-Joseph-de-Lepage,_Quebec (URI: http://dbpedia.org/resource/Saint-Joseph-de-Lepage,_Quebec, ID: 36175, Language: English)\n",
      "  4. Saint-Valérien-de-Milton,_Quebec (URI: http://dbpedia.org/resource/Saint-Valérien-de-Milton,_Quebec, ID: 17181, Language: English)\n",
      "  5. Saint-Bernard-de-Michaudville,_Quebec (URI: http://dbpedia.org/resource/Saint-Bernard-de-Michaudville,_Quebec, ID: 20105, Language: English)\n"
     ]
    }
   ],
   "source": [
    "# Create the enhanced dataframe with entity names and URIs\n",
    "entity_alignment_data = []\n",
    "\n",
    "print(\"Creating enhanced dataframe with entity names and URIs...\")\n",
    "for _, row in df.iterrows():\n",
    "    index_id = row['index']\n",
    "    candidates = row['candidates']\n",
    "    \n",
    "    # Get index entity information (from KG1 - French)\n",
    "    if index_id in kg1_mappings:\n",
    "        index_name = kg1_mappings[index_id]['name']\n",
    "        index_uri = kg1_mappings[index_id]['uri']\n",
    "        index_language = 'French'\n",
    "    else:\n",
    "        index_name = f\"Unknown_Entity_{index_id}\"\n",
    "        index_uri = f\"Unknown_URI_{index_id}\"\n",
    "        index_language = 'Unknown'\n",
    "    \n",
    "    # Get candidate entities information (from KG2 - English)\n",
    "    candidate_info = []\n",
    "    for candidate_id in candidates:\n",
    "        if candidate_id in kg2_mappings:\n",
    "            candidate_name = kg2_mappings[candidate_id]['name']\n",
    "            candidate_uri = kg2_mappings[candidate_id]['uri']\n",
    "            candidate_language = 'English'\n",
    "        else:\n",
    "            candidate_name = f\"Unknown_Entity_{candidate_id}\"\n",
    "            candidate_uri = f\"Unknown_URI_{candidate_id}\"\n",
    "            candidate_language = 'Unknown'\n",
    "        \n",
    "        candidate_info.append([candidate_name, candidate_uri, candidate_id, candidate_language])\n",
    "    \n",
    "    # Create the row structure: [index_name, index_uri, index_id, language, candidates_list]\n",
    "    entity_alignment_data.append({\n",
    "        'index_name': index_name,\n",
    "        'index_uri': index_uri,\n",
    "        'index_id': index_id,\n",
    "        'index_language': index_language,\n",
    "        'candidates': candidate_info,\n",
    "        'num_candidates': len(candidate_info)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "enhanced_df = pd.DataFrame(entity_alignment_data)\n",
    "\n",
    "print(f\"Enhanced DataFrame shape: {enhanced_df.shape}\")\n",
    "print(\"\\nFirst few rows of enhanced DataFrame:\")\n",
    "print(enhanced_df[['index_name', 'index_uri', 'index_id', 'index_language', 'num_candidates']].head())\n",
    "\n",
    "# Show a sample of candidates for the first entity\n",
    "print(f\"\\nSample candidates for first entity ('{enhanced_df.iloc[0]['index_name']}'):\")\n",
    "for i, candidate in enumerate(enhanced_df.iloc[0]['candidates'][:5]):  # Show first 5 candidates\n",
    "    print(f\"  {i+1}. {candidate[0]} (URI: {candidate[1]}, ID: {candidate[2]}, Language: {candidate[3]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataframe saved to: processed_data/entity_alignment_with_names_and_uris.csv\n",
      "Flattened dataframe saved to: processed_data/entity_alignment_flattened_with_uris.csv\n",
      "Flattened DataFrame shape: (90000, 8)\n",
      "\n",
      "First few rows of flattened DataFrame:\n",
      "                  index_name  \\\n",
      "0  Saint-Joseph-de-Coleraine   \n",
      "1  Saint-Joseph-de-Coleraine   \n",
      "2  Saint-Joseph-de-Coleraine   \n",
      "3  Saint-Joseph-de-Coleraine   \n",
      "4  Saint-Joseph-de-Coleraine   \n",
      "5  Saint-Joseph-de-Coleraine   \n",
      "6  Saint-Joseph-de-Coleraine   \n",
      "7  Saint-Joseph-de-Coleraine   \n",
      "8  Saint-Joseph-de-Coleraine   \n",
      "9  Saint-Joseph-de-Coleraine   \n",
      "\n",
      "                                           index_uri  index_id index_language  \\\n",
      "0  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "1  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "2  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "3  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "4  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "5  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "6  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "7  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "8  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "9  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "\n",
      "                          candidate_name  \\\n",
      "0      Saint-Joseph-de-Coleraine,_Quebec   \n",
      "1                 Saint-Joseph-de-Beauce   \n",
      "2         Saint-Joseph-de-Lepage,_Quebec   \n",
      "3       Saint-Valérien-de-Milton,_Quebec   \n",
      "4  Saint-Bernard-de-Michaudville,_Quebec   \n",
      "5     Saint-Joseph-de-Kamouraska,_Quebec   \n",
      "6         Saint-Paul-de-Montminy,_Quebec   \n",
      "7       Saint-Isidore-de-Clifton,_Quebec   \n",
      "8         Saint-Paul-de-la-Croix,_Quebec   \n",
      "9               Saint-Edmond-de-Grantham   \n",
      "\n",
      "                                       candidate_uri  candidate_id  \\\n",
      "0  http://dbpedia.org/resource/Saint-Joseph-de-Co...         10500   \n",
      "1  http://dbpedia.org/resource/Saint-Joseph-de-Be...         33775   \n",
      "2  http://dbpedia.org/resource/Saint-Joseph-de-Le...         36175   \n",
      "3  http://dbpedia.org/resource/Saint-Valérien-de-...         17181   \n",
      "4  http://dbpedia.org/resource/Saint-Bernard-de-M...         20105   \n",
      "5  http://dbpedia.org/resource/Saint-Joseph-de-Ka...         32176   \n",
      "6  http://dbpedia.org/resource/Saint-Paul-de-Mont...         11787   \n",
      "7  http://dbpedia.org/resource/Saint-Isidore-de-C...         38085   \n",
      "8  http://dbpedia.org/resource/Saint-Paul-de-la-C...         37257   \n",
      "9  http://dbpedia.org/resource/Saint-Edmond-de-Gr...         33904   \n",
      "\n",
      "  candidate_language  \n",
      "0            English  \n",
      "1            English  \n",
      "2            English  \n",
      "3            English  \n",
      "4            English  \n",
      "5            English  \n",
      "6            English  \n",
      "7            English  \n",
      "8            English  \n",
      "9            English  \n"
     ]
    }
   ],
   "source": [
    "# Save the enhanced dataframe to CSV\n",
    "csv_file_path = 'processed_data/entity_alignment_with_names_and_uris.csv'\n",
    "\n",
    "# For CSV export, we need to convert the candidates list to a string format\n",
    "# that can be properly saved and loaded later\n",
    "enhanced_df_for_csv = enhanced_df.copy()\n",
    "enhanced_df_for_csv['candidates_str'] = enhanced_df_for_csv['candidates'].apply(str)\n",
    "\n",
    "# Save the main columns (excluding the original candidates list which is complex)\n",
    "columns_to_save = ['index_name', 'index_uri', 'index_id', 'index_language', 'candidates_str', 'num_candidates']\n",
    "enhanced_df_for_csv[columns_to_save].to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Enhanced dataframe saved to: {csv_file_path}\")\n",
    "\n",
    "# Also create a more readable version where each candidate is on a separate row\n",
    "flattened_data = []\n",
    "for _, row in enhanced_df.iterrows():\n",
    "    index_name = row['index_name']\n",
    "    index_uri = row['index_uri']\n",
    "    index_id = row['index_id']\n",
    "    index_language = row['index_language']\n",
    "    \n",
    "    for candidate in row['candidates']:\n",
    "        candidate_name, candidate_uri, candidate_id, candidate_language = candidate\n",
    "        flattened_data.append({\n",
    "            'index_name': index_name,\n",
    "            'index_uri': index_uri,\n",
    "            'index_id': index_id,\n",
    "            'index_language': index_language,\n",
    "            'candidate_name': candidate_name,\n",
    "            'candidate_uri': candidate_uri,\n",
    "            'candidate_id': candidate_id,\n",
    "            'candidate_language': candidate_language\n",
    "        })\n",
    "\n",
    "flattened_df = pd.DataFrame(flattened_data)\n",
    "flattened_csv_path = 'processed_data/entity_alignment_flattened_with_uris.csv'\n",
    "flattened_df.to_csv(flattened_csv_path, index=False)\n",
    "\n",
    "print(f\"Flattened dataframe saved to: {flattened_csv_path}\")\n",
    "print(f\"Flattened DataFrame shape: {flattened_df.shape}\")\n",
    "print(\"\\nFirst few rows of flattened DataFrame:\")\n",
    "print(flattened_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUMMARY STATISTICS ===\n",
      "Total number of index entities (French): 4500\n",
      "Total number of candidate pairs: 90000\n",
      "Average candidates per index entity: 20.00\n",
      "\n",
      "Unknown entities:\n",
      "  - Unknown index entities: 0\n",
      "  - Unknown candidate entities: 0\n",
      "\n",
      "Files created in 'processed_data' folder:\n",
      "  1. entity_alignment_with_names_and_uris.csv - Main dataframe with candidates as string\n",
      "  2. entity_alignment_flattened_with_uris.csv - Each index-candidate pair as separate row\n",
      "\n",
      "=== SAMPLE DATA STRUCTURE ===\n",
      "Format: [entity_name, entity_uri, entity_id, language] [[candidate_1_name, candidate_1_uri, candidate_1_id, language], ...]\n",
      "Example:\n",
      "['Saint-Joseph-de-Coleraine', 'http://fr.dbpedia.org/resource/Saint-Joseph-de-Coleraine', 0, 'French']\n",
      "Candidates: [['Saint-Joseph-de-Coleraine,_Quebec', 'http://dbpedia.org/resource/Saint-Joseph-de-Coleraine,_Quebec', 10500, 'English'], ['Saint-Joseph-de-Beauce', 'http://dbpedia.org/resource/Saint-Joseph-de-Beauce', 33775, 'English']]...\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total number of index entities (French): {len(enhanced_df)}\")\n",
    "print(f\"Total number of candidate pairs: {len(flattened_df)}\")\n",
    "print(f\"Average candidates per index entity: {flattened_df.shape[0] / enhanced_df.shape[0]:.2f}\")\n",
    "\n",
    "# Check for any unknown entities\n",
    "unknown_index = enhanced_df[enhanced_df['index_language'] == 'Unknown'].shape[0]\n",
    "unknown_candidates = flattened_df[flattened_df['candidate_language'] == 'Unknown'].shape[0]\n",
    "\n",
    "print(f\"\\nUnknown entities:\")\n",
    "print(f\"  - Unknown index entities: {unknown_index}\")\n",
    "print(f\"  - Unknown candidate entities: {unknown_candidates}\")\n",
    "\n",
    "print(f\"\\nFiles created in 'processed_data' folder:\")\n",
    "print(f\"  1. entity_alignment_with_names_and_uris.csv - Main dataframe with candidates as string\")\n",
    "print(f\"  2. entity_alignment_flattened_with_uris.csv - Each index-candidate pair as separate row\")\n",
    "\n",
    "# Show sample of the data structure requested\n",
    "print(f\"\\n=== SAMPLE DATA STRUCTURE ===\")\n",
    "sample_entity = enhanced_df.iloc[0]\n",
    "print(f\"Format: [entity_name, entity_uri, entity_id, language] [[candidate_1_name, candidate_1_uri, candidate_1_id, language], ...]\")\n",
    "print(f\"Example:\")\n",
    "print(f\"['{sample_entity['index_name']}', '{sample_entity['index_uri']}', {sample_entity['index_id']}, '{sample_entity['index_language']}']\")\n",
    "print(f\"Candidates: {sample_entity['candidates'][:2]}...\")  # Show first 2 candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama is running and gemma:4b model is available\n",
      "Ready to start entity categorization!\n"
     ]
    }
   ],
   "source": [
    "# Entity Categorization using Ollama gemma:4b\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def query_ollama_gemma(entity_name, entity_uri, max_retries=3, delay=1):\n",
    "    \"\"\"\n",
    "    Query Ollama gemma:4b model to categorize an entity\n",
    "    \n",
    "    Args:\n",
    "        entity_name (str): Name of the entity\n",
    "        entity_uri (str): URI of the entity for additional context\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        delay (int): Delay between retries in seconds\n",
    "    \n",
    "    Returns:\n",
    "        str: Category of the entity\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the categories\n",
    "    categories = [\"Person\", \"Place\", \"Event\", \"building/place\", \"Creative Work\", \"uncertain\"]\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"You are an expert entity classifier. Given an entity name and its URI, classify it into one of these categories:\n",
    "- Person: Individual people, historical figures, celebrities, etc.\n",
    "- Place: Geographical locations, cities, countries, regions, etc.\n",
    "- Event: Historical events, competitions, festivals, wars, etc.\n",
    "- building/place: Specific buildings, monuments, structures, venues, etc.\n",
    "- Creative Work: Books, movies, songs, artworks, publications, etc.\n",
    "- uncertain: When the category is unclear or ambiguous\n",
    "\n",
    "Entity Name: {entity_name}\n",
    "Entity URI: {entity_uri}\n",
    "\n",
    "Based on the entity name and URI context, classify this entity. Respond with ONLY one of these exact categories: {', '.join(categories)}\n",
    "\n",
    "Category:\"\"\"\n",
    "\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9,\n",
    "            \"num_predict\": 50\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, json=data, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            category = result.get('response', '').strip()\n",
    "            \n",
    "            # Clean and validate the response\n",
    "            category = category.replace('.', '').replace('\\n', '').strip()\n",
    "            \n",
    "            # Check if the response matches one of our categories\n",
    "            for cat in categories:\n",
    "                if cat.lower() in category.lower():\n",
    "                    return cat\n",
    "            \n",
    "            # If no exact match, try to map common variations\n",
    "            category_lower = category.lower()\n",
    "            if any(word in category_lower for word in ['person', 'people', 'individual', 'human']):\n",
    "                return \"Person\"\n",
    "            elif any(word in category_lower for word in ['place', 'location', 'city', 'country', 'region']):\n",
    "                return \"Place\"\n",
    "            elif any(word in category_lower for word in ['event', 'competition', 'festival', 'war']):\n",
    "                return \"Event\"\n",
    "            elif any(word in category_lower for word in ['building', 'structure', 'monument', 'venue']):\n",
    "                return \"building/place\"\n",
    "            elif any(word in category_lower for word in ['creative', 'work', 'book', 'movie', 'song', 'art']):\n",
    "                return \"Creative Work\"\n",
    "            else:\n",
    "                return \"uncertain\"\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed for {entity_name}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay * (attempt + 1))\n",
    "            else:\n",
    "                logger.error(f\"All attempts failed for {entity_name}\")\n",
    "                return \"uncertain\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error for {entity_name}: {str(e)}\")\n",
    "            return \"uncertain\"\n",
    "    \n",
    "    return \"uncertain\"\n",
    "\n",
    "# Test the connection to Ollama\n",
    "def test_ollama_connection():\n",
    "    \"\"\"Test if Ollama is running and gemma:4b is available\"\"\"\n",
    "    try:\n",
    "        url = \"http://localhost:11434/api/tags\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        models = response.json().get('models', [])\n",
    "        gemma_available = any('gemma3:1b' in model.get('name', '') for model in models)\n",
    "        \n",
    "        if gemma_available:\n",
    "            print(\"✅ Ollama is running and gemma:4b model is available\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ gemma:4b model not found. Available models:\")\n",
    "            for model in models:\n",
    "                print(f\"  - {model.get('name', 'Unknown')}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Failed to connect to Ollama: {str(e)}\")\n",
    "        print(\"Make sure Ollama is running on localhost:11434\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "if test_ollama_connection():\n",
    "    print(\"Ready to start entity categorization!\")\n",
    "else:\n",
    "    print(\"Please start Ollama and ensure gemma3:1b model is available before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity categorization functions loaded!\n",
      "Choose an option:\n",
      "1. Run test with 5 sample entities: test_categorization_sample(enhanced_df)\n",
      "2. Start full categorization: categorize_entities_batch(enhanced_df)\n",
      "3. Start with custom batch size: categorize_entities_batch(enhanced_df, batch_size=10)\n"
     ]
    }
   ],
   "source": [
    "# Main batch processing functions for entity categorization\n",
    "\n",
    "def categorize_entities_batch(enhanced_df, batch_size=50, save_interval=100):\n",
    "    \"\"\"\n",
    "    Categorize entities using Ollama gemma:4b model in batches\n",
    "    \n",
    "    Args:\n",
    "        enhanced_df (pd.DataFrame): DataFrame with entity information\n",
    "        batch_size (int): Number of entities to process in each batch\n",
    "        save_interval (int): Save progress every N entities\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with categorized entities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a unique list of index entities to categorize\n",
    "    unique_entities = enhanced_df[['index_name', 'index_uri', 'index_id', 'index_language']].drop_duplicates()\n",
    "    \n",
    "    print(f\"Starting categorization of {len(unique_entities)} unique entities\")\n",
    "    print(f\"Batch size: {batch_size}, Save interval: {save_interval}\")\n",
    "    \n",
    "    # Initialize results list\n",
    "    categorized_entities = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process entities in batches\n",
    "    for i in tqdm(range(0, len(unique_entities), batch_size), desc=\"Processing batches\"):\n",
    "        batch = unique_entities.iloc[i:i+batch_size]\n",
    "        \n",
    "        for _, entity in batch.iterrows():\n",
    "            entity_name = entity['index_name']\n",
    "            entity_uri = entity['index_uri']\n",
    "            entity_id = entity['index_id']\n",
    "            entity_language = entity['index_language']\n",
    "            \n",
    "            # Query Ollama for categorization\n",
    "            category = query_ollama_gemma(entity_name, entity_uri)\n",
    "            \n",
    "            # Store results\n",
    "            categorized_entities.append({\n",
    "                'entity_name': entity_name,\n",
    "                'entity_uri': entity_uri,\n",
    "                'entity_id': entity_id,\n",
    "                'entity_language': entity_language,\n",
    "                'entity_category': category,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if processed_count % save_interval == 0:\n",
    "                temp_df = pd.DataFrame(categorized_entities)\n",
    "                temp_csv_path = f'processed_data/categorised_entities_by_gemma_temp_{processed_count}.csv'\n",
    "                temp_df.to_csv(temp_csv_path, index=False)\n",
    "                \n",
    "                elapsed_time = time.time() - start_time\n",
    "                entities_per_sec = processed_count / elapsed_time\n",
    "                print(f\"\\n💾 Saved intermediate results: {processed_count}/{len(unique_entities)} entities\")\n",
    "                print(f\"⏱️  Processing speed: {entities_per_sec:.2f} entities/second\")\n",
    "                print(f\"📊 Current category breakdown:\")\n",
    "                print(temp_df['entity_category'].value_counts().to_string())\n",
    "                print()\n",
    "        \n",
    "        # Add a small delay between batches to avoid overwhelming the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    final_df = pd.DataFrame(categorized_entities)\n",
    "    \n",
    "    # Save final results\n",
    "    final_csv_path = 'processed_data/categorised_entities_by_gemma.csv'\n",
    "    final_df.to_csv(final_csv_path, index=False)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    elapsed_time = time.time() - start_time\n",
    "    entities_per_sec = processed_count / elapsed_time\n",
    "    \n",
    "    print(f\"\\n🎉 Categorization completed!\")\n",
    "    print(f\"📁 Final results saved to: {final_csv_path}\")\n",
    "    print(f\"📊 Total entities processed: {processed_count}\")\n",
    "    print(f\"⏱️  Total time: {elapsed_time:.2f} seconds ({entities_per_sec:.2f} entities/second)\")\n",
    "    print(f\"\\n📈 Final category breakdown:\")\n",
    "    print(final_df['entity_category'].value_counts().to_string())\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Test with a small sample first (optional)\n",
    "def test_categorization_sample(enhanced_df, sample_size=5):\n",
    "    \"\"\"Test categorization with a small sample\"\"\"\n",
    "    print(f\"Testing categorization with {sample_size} entities...\")\n",
    "    \n",
    "    sample_entities = enhanced_df.head(sample_size)\n",
    "    test_results = []\n",
    "    \n",
    "    for _, entity in sample_entities.iterrows():\n",
    "        entity_name = entity['index_name']\n",
    "        entity_uri = entity['index_uri']\n",
    "        \n",
    "        print(f\"🔍 Categorizing: {entity_name}\")\n",
    "        category = query_ollama_gemma(entity_name, entity_uri)\n",
    "        print(f\"📝 Category: {category}\")\n",
    "        \n",
    "        test_results.append({\n",
    "            'entity_name': entity_name,\n",
    "            'entity_uri': entity_uri,\n",
    "            'entity_category': category\n",
    "        })\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    test_df = pd.DataFrame(test_results)\n",
    "    print(f\"\\n📊 Test results:\")\n",
    "    print(test_df[['entity_name', 'entity_category']].to_string(index=False))\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "print(\"Entity categorization functions loaded!\")\n",
    "print(\"Choose an option:\")\n",
    "print(\"1. Run test with 5 sample entities: test_categorization_sample(enhanced_df)\")\n",
    "print(\"2. Start full categorization: categorize_entities_batch(enhanced_df)\")\n",
    "print(\"3. Start with custom batch size: categorize_entities_batch(enhanced_df, batch_size=10)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting categorization of 4500 unique entities\n",
      "Batch size: 10, Save interval: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 4/450 [01:51<3:25:39, 27.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved intermediate results: 50/4500 entities\n",
      "⏱️  Processing speed: 0.36 entities/second\n",
      "📊 Current category breakdown:\n",
      "entity_category\n",
      "Creative Work    24\n",
      "Person           16\n",
      "uncertain         7\n",
      "Place             2\n",
      "Event             1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 8/450 [04:05<3:45:46, 30.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m sock.connect(sa)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Execute entity categorization\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Uncomment the line below to run the categorization you want:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Option 3: Run with smaller batch size (good for testing)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m categorized_df = \u001b[43mcategorize_entities_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43menhanced_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Ready to categorize entities!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUncomment one of the options above to start the categorization process.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mcategorize_entities_batch\u001b[39m\u001b[34m(enhanced_df, batch_size, save_interval)\u001b[39m\n\u001b[32m     37\u001b[39m entity_language = entity[\u001b[33m'\u001b[39m\u001b[33mindex_language\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Query Ollama for categorization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m category = \u001b[43mquery_ollama_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     43\u001b[39m categorized_entities.append({\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mentity_name\u001b[39m\u001b[33m'\u001b[39m: entity_name,\n\u001b[32m     45\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mentity_uri\u001b[39m\u001b[33m'\u001b[39m: entity_uri,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m: time.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mquery_ollama_gemma\u001b[39m\u001b[34m(entity_name, entity_uri, max_retries, delay)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m         response.raise_for_status()\n\u001b[32m     65\u001b[39m         result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\connection.py:494\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers.items():\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1298\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1058\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1056\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1061\u001b[39m \n\u001b[32m   1062\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1064\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1065\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1066\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:996\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    998\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\connection.py:325\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    327\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mself\u001b[39m._has_connected_to_proxy = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:81\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     79\u001b[39m         err = _\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m             \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:499\u001b[39m, in \u001b[36msocket.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m, _ss=_socket.socket):\n\u001b[32m    496\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    497\u001b[39m     _ss.close(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._io_refs <= \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execute entity categorization\n",
    "# Uncomment the line below to run the categorization you want:\n",
    "\n",
    "# Option 1: Test with 5 sample entities first\n",
    "# test_result = test_categorization_sample(enhanced_df, sample_size=5)\n",
    "\n",
    "# Option 2: Run full categorization with default settings (50 entities per batch)\n",
    "# categorized_df = categorize_entities_batch(enhanced_df)\n",
    "\n",
    "# Option 3: Run with smaller batch size (good for testing)\n",
    "categorized_df = categorize_entities_batch(enhanced_df, batch_size=10, save_interval=50)\n",
    "\n",
    "print(\"🚀 Ready to categorize entities!\")\n",
    "print(\"Uncomment one of the options above to start the categorization process.\")\n",
    "print(\"Make sure Ollama is running with gemma:4b model before proceeding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          index_name  index_id\n",
      "0                          Saint-Joseph-de-Coleraine         0\n",
      "1                                      Self_Portrait         1\n",
      "2  Alliance_des_libéraux_et_des_démocrates_pour_l...         2\n",
      "3                                             Wallon         3\n",
      "4                                            Android         4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, usecols=['index_name', 'index_id'])\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          index_name  index_id\n",
      "0                          Saint-Joseph-de-Coleraine         0\n",
      "1                                      Self_Portrait         1\n",
      "2  Alliance_des_libéraux_et_des_démocrates_pour_l...         2\n",
      "3                                             Wallon         3\n",
      "4                                            Android         4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, usecols=['index_name', 'index_id'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\entity_names_id.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 4500 rows from 'C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris_category.csv'.\n",
      "Category 'Person': Created 4 batches.\n",
      "Category 'buildings/places': Created 1 batches.\n",
      "Category 'creative_work': Created 2 batches.\n",
      "Category 'event': Created 1 batches.\n",
      "Category 'place': Created 2 batches.\n",
      "Category 'uncertain': Created 2 batches.\n",
      "\n",
      "Saving batches to directory: 'C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\categories'\n",
      "-> Saving category: Person\n",
      "-> Saving category: buildings/places\n",
      "-> Saving category: creative_work\n",
      "-> Saving category: event\n",
      "-> Saving category: place\n",
      "-> Saving category: uncertain\n",
      "\n",
      "All batches have been saved successfully.\n",
      "\n",
      "--- Example Output ---\n",
      "Check the 'categories' folder.\n",
      "You should see files like:\n",
      "  - Person_batch_1.csv\n",
      "  - place_batch_1.csv\n",
      "  - buildings_places_batch_1.csv\n",
      "  - etc.\n"
     ]
    }
   ],
   "source": [
    "def batch_entities_by_category(csv_path: str, batch_size: int = 500):\n",
    "    \"\"\"\n",
    "    Loads a CSV, groups entities by category, and creates batches for each category.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): The path to the input CSV file.\n",
    "        batch_size (int): The number of entities per batch.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are category names and values are lists\n",
    "              of pandas DataFrames, with each DataFrame representing a batch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Successfully loaded {len(df)} rows from '{csv_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_path}' was not found.\")\n",
    "        return None\n",
    "\n",
    "    grouped_by_category = df.groupby('category')\n",
    "    all_batches = {}\n",
    "\n",
    "    for category_name, category_df in grouped_by_category:\n",
    "        category_batches = []\n",
    "        num_entities = len(category_df)\n",
    "        for i in range(0, num_entities, batch_size):\n",
    "            batch_df = category_df.iloc[i : i + batch_size]\n",
    "            category_batches.append(batch_df)\n",
    "        all_batches[category_name] = category_batches\n",
    "        print(f\"Category '{category_name}': Created {len(category_batches)} batches.\")\n",
    "\n",
    "    return all_batches\n",
    "\n",
    "# --- 3. NEW: Function to Save Batches to CSV Files ---\n",
    "\n",
    "def save_batches_to_csv(batched_data: dict, output_dir: str):\n",
    "    \"\"\"\n",
    "    Saves each batch DataFrame to a separate CSV file in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        batched_data (dict): The dictionary of batches generated by batch_entities_by_category.\n",
    "        output_dir (str): The directory where the CSV files will be saved.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving batches to directory: '{output_dir}'\")\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through each category and its list of batches\n",
    "    for category, batches in batched_data.items():\n",
    "        # Sanitize the category name to make it a valid filename\n",
    "        # This replaces slashes (like in 'buildings/places') with underscores\n",
    "        sanitized_category_name = category.replace('/', '_')\n",
    "        \n",
    "        print(f\"-> Saving category: {category}\")\n",
    "        \n",
    "        # Iterate through each batch in the list, using enumerate for a batch ID\n",
    "        for i, batch_df in enumerate(batches):\n",
    "            # The batch ID will be 1, 2, 3, ... instead of 0, 1, 2, ...\n",
    "            batch_id = i + 1\n",
    "            \n",
    "            # Construct the filename\n",
    "            filename = f\"{sanitized_category_name}_batch_{batch_id}.csv\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Save the batch DataFrame to a CSV file\n",
    "            # index=False prevents pandas from writing the DataFrame index as a column\n",
    "            batch_df.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(\"\\nAll batches have been saved successfully.\")\n",
    "\n",
    "\n",
    "# --- 4. Running the Code ---\n",
    "\n",
    "\n",
    "    # Step 2: Process the CSV and create the batches in memory\n",
    "batched_data = batch_entities_by_category(r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris_category.csv\", batch_size=500)\n",
    "\n",
    "    # Step 3: Save the generated batches to files, if any were created\n",
    "if batched_data:\n",
    "    save_batches_to_csv(batched_data, output_dir=r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\categories\")\n",
    "\n",
    "    print(\"\\n--- Example Output ---\")\n",
    "    print(f\"Check the 'categories' folder.\")\n",
    "    print(\"You should see files like:\")\n",
    "    print(\"  - Person_batch_1.csv\")\n",
    "    print(\"  - place_batch_1.csv\")\n",
    "    print(\"  - buildings_places_batch_1.csv\")\n",
    "    print(\"  - etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "SOURCE_FILE = 'mvp-kg-alignment\\processed_data\\entity_name_id_category.csv'\n",
    "TARGET_FILE = 'mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris.csv'\n",
    "OUTPUT_FILE = 'mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris_category.csv'\n",
    "\n",
    "def merge_category_data(source_path, target_path, output_path):\n",
    "    \"\"\"\n",
    "    Merges a category column from a source CSV into a target CSV.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Path to the CSV with (index_id, category).\n",
    "        target_path (str): Path to the main CSV file.\n",
    "        output_path (str): Path to save the merged CSV file.\n",
    "    \"\"\"\n",
    "    # --- 1. Load the CSV files into pandas DataFrames ---\n",
    "    try:\n",
    "        source_df = pd.read_csv(source_path)\n",
    "        target_df = pd.read_csv(target_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find a file. {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Successfully loaded source and target files.\")\n",
    "    \n",
    "    # --- 2. Prepare the source data ---\n",
    "    # We only need the 'index_id' and 'category' columns for the merge.\n",
    "    # This avoids potential conflicts with other columns like 'index_name'.\n",
    "    category_data = source_df[['index_id', 'category']]\n",
    "\n",
    "    # --- 3. Merge the two DataFrames ---\n",
    "    # We use a 'left' merge to keep all rows from the target_df.\n",
    "    # If an index_id from the target file doesn't exist in the source file,\n",
    "    # the 'category' for that row will be empty (NaN).\n",
    "    merged_df = pd.merge(target_df, category_data, on='index_id', how='left')\n",
    "    \n",
    "    # Fill any missing categories with an empty string or 'N/A' if preferred\n",
    "    merged_df['category'].fillna('', inplace=True)\n",
    "    \n",
    "    # --- 4. Reorder columns to the desired structure ---\n",
    "    # The new 'category' column is currently at the end. We move it.\n",
    "    desired_order = [\n",
    "        'index_name',\n",
    "        'index_uri',\n",
    "        'index_id',\n",
    "        'index_language',\n",
    "        'category',  # <-- Inserted here\n",
    "        'candidates_str',\n",
    "        'num_candidates'\n",
    "    ]\n",
    "    \n",
    "    # Check if all desired columns exist in the merged dataframe\n",
    "    # This is a safeguard in case of unexpected column names\n",
    "    final_df = merged_df[desired_order]\n",
    "    \n",
    "    # --- 5. Save the result to a new CSV file ---\n",
    "    # index=False prevents pandas from writing the DataFrame index as a column\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Successfully merged data and saved to '{output_path}'\")\n",
    "    print(f\"\\nFinal DataFrame preview:\\n{final_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded source and target files.\n",
      "Successfully merged data and saved to 'C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris_category.csv'\n",
      "\n",
      "Final DataFrame preview:\n",
      "                                          index_name  \\\n",
      "0                          Saint-Joseph-de-Coleraine   \n",
      "1                                      Self_Portrait   \n",
      "2  Alliance_des_libéraux_et_des_démocrates_pour_l...   \n",
      "3                                             Wallon   \n",
      "4                                            Android   \n",
      "\n",
      "                                           index_uri  index_id index_language  \\\n",
      "0  http://fr.dbpedia.org/resource/Saint-Joseph-de...         0         French   \n",
      "1       http://fr.dbpedia.org/resource/Self_Portrait         1         French   \n",
      "2  http://fr.dbpedia.org/resource/Alliance_des_li...         2         French   \n",
      "3              http://fr.dbpedia.org/resource/Wallon         3         French   \n",
      "4             http://fr.dbpedia.org/resource/Android         4         French   \n",
      "\n",
      "        category                                     candidates_str  \\\n",
      "0          place  [['Saint-Joseph-de-Coleraine,_Quebec', 'http:/...   \n",
      "1  creative_work  [['Self_Portrait_(Bob_Dylan_album)', 'http://d...   \n",
      "2      uncertain  [['Alliance_of_Liberals_and_Democrats_for_Euro...   \n",
      "3      uncertain  [['Walloon_language', 'http://dbpedia.org/reso...   \n",
      "4  creative_work  [['Paranoid_Android', 'http://dbpedia.org/reso...   \n",
      "\n",
      "   num_candidates  \n",
      "0              20  \n",
      "1              20  \n",
      "2              20  \n",
      "3              20  \n",
      "4              20  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DavidCaraman\\AppData\\Local\\Temp\\ipykernel_18700\\2121028299.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['category'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "SOURCE_FILE = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\entity_name_id_category.csv\"\n",
    "TARGET_FILE = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris.csv\"\n",
    "OUTPUT_FILE = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\processed_data\\fr_en_train_cand_list_20_entity_alignment_with_names_and_uris_category.csv\"\n",
    "merge_category_data(SOURCE_FILE, TARGET_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
