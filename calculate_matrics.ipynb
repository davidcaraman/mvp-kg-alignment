{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\building_places_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Eroare la procesarea fișierului C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\building_places_batch_1_aligned.csv: Cannot convert non-finite values (NA or inf) to integer\n",
      "Eroare la procesarea fișierului C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\creative_work_batch_1_aligned.csv: Cannot convert non-finite values (NA or inf) to integer\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\creative_work_batch_2_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\event_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\event_batch_1_aligned.csv\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_1_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_2_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_2_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_3_alignment.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_3_alignment.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_4_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_4_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_1_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_2_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_2_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_1_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_2_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Eroare la procesarea fișierului C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_2_aligned.csv: Cannot convert non-finite values (NA or inf) to integer\n",
      "Datele concatenate și standardizate au fost salvate în: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\entity_alignment.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def concatenate_and_standardize_csv_files(folder_path, output_filename=\"entity_alignment.csv\"):\n",
    "    \"\"\"\n",
    "    Concatenates multiple CSV files from a specified folder, standardizes column names,\n",
    "    sorts by 'index_id', and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "        output_filename (str): The name of the output CSV file.\n",
    "    \"\"\"\n",
    "    standard_columns = ['index_id', 'candidate_id', 'confidence_score', 'need_judge']\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Get all CSV file paths in the specified folder\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.csv')]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"Nu s-au găsit fișiere CSV în directorul: {folder_path}. Asigurați-vă că fișierele sunt prezente.\")\n",
    "        return\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            # Read the file with no header initially to find the correct header row\n",
    "            df_raw = pd.read_csv(file_path, header=None, encoding='utf-8')\n",
    "\n",
    "            # Look for the header row containing all standard column names\n",
    "            header_row_index = -1\n",
    "            for i, row in df_raw.iterrows():\n",
    "                # Convert row to string and check for standard column names\n",
    "                row_values = row.dropna().astype(str).tolist()\n",
    "                row_str = \" \".join(row_values).lower()\n",
    "                \n",
    "                # Check if all standard column names (or close variations) are present\n",
    "                if all(any(col_keyword in s for s in row_values) for col_keyword in [col.lower() for col in standard_columns]):\n",
    "                    header_row_index = i\n",
    "                    break\n",
    "            \n",
    "            if header_row_index == -1:\n",
    "                print(f\"Avertisment: Antetul standard nu a fost găsit explicit în {file_path}. Se încearcă inferența coloanelor.\")\n",
    "                # Fallback: Try to infer based on typical data start\n",
    "                potential_data_start_row = -1\n",
    "                for i, row in df_raw.iterrows():\n",
    "                    first_val = row.dropna().iloc[0] if not row.dropna().empty else None\n",
    "                    if pd.notna(first_val) and (isinstance(first_val, (int, float)) or str(first_val).replace('.', '').isdigit() or str(first_val).lower() in ['true', 'false']):\n",
    "                        potential_data_start_row = i\n",
    "                        break\n",
    "                \n",
    "                if potential_data_start_row != -1:\n",
    "                    df = pd.read_csv(file_path, header=potential_data_start_row, encoding='utf-8')\n",
    "                    # Assume column order based on typical CSV structure if header not found\n",
    "                    if len(df.columns) >= len(standard_columns):\n",
    "                        df.rename(columns={df.columns[0]: 'index_id',\n",
    "                                           df.columns[1]: 'candidate_id',\n",
    "                                           df.columns[2]: 'confidence_score',\n",
    "                                           df.columns[3]: 'need_judge'}, inplace=True)\n",
    "                        df = df[standard_columns]\n",
    "                    else:\n",
    "                        print(f\"Se omite {file_path} din cauza numărului insuficient de coloane după inferență.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Se omite {file_path} deoarece nu a fost găsit un antet sau un început de date identificabil.\")\n",
    "                    continue\n",
    "            else:\n",
    "                # Read the file again with the identified header row\n",
    "                df = pd.read_csv(file_path, header=header_row_index, encoding='utf-8')\n",
    "\n",
    "                # Standardize column names based on content\n",
    "                col_mapping = {}\n",
    "                for std_col in standard_columns:\n",
    "                    # Find the first column that contains the standard column name (case-insensitive)\n",
    "                    matching_cols = [col for col in df.columns if std_col.lower() in str(col).lower()]\n",
    "                    if matching_cols and matching_cols[0] not in col_mapping:\n",
    "                        col_mapping[matching_cols[0]] = std_col\n",
    "                    else:\n",
    "                        print(f\"Avertisment: Coloana '{std_col}' nu a fost găsită sau este duplicată în {file_path}. S-ar putea să existe probleme de mapare.\")\n",
    "\n",
    "                # Apply mapping and select only standard columns\n",
    "                df.rename(columns=col_mapping, inplace=True)\n",
    "                df = df[[col for col in standard_columns if col in df.columns]]\n",
    "                \n",
    "                if len(df.columns) != len(standard_columns):\n",
    "                    print(f\"Avertisment: După standardizare, fișierul {file_path} nu conține toate coloanele așteptate: {standard_columns}. Coloane actuale: {list(df.columns)}\")\n",
    "\n",
    "            # Ensure all standard columns are present, fill missing with NaN\n",
    "            for col in standard_columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            df = df[standard_columns] # Reorder columns to be consistent\n",
    "\n",
    "            # Clean up and convert data types\n",
    "            df['index_id'] = pd.to_numeric(df['index_id'], errors='coerce')\n",
    "            df['candidate_id'] = pd.to_numeric(df['candidate_id'], errors='coerce')\n",
    "            df['candidate_id'] = df['candidate_id'].astype(int)\n",
    "            df['confidence_score'] = pd.to_numeric(df['confidence_score'], errors='coerce')\n",
    "            \n",
    "            # Convert need_judge to boolean, handling various string representations\n",
    "            df['need_judge'] = df['need_judge'].astype(str).str.lower().isin(['true', '1', 'yes'])\n",
    "            \n",
    "            # Drop rows where index_id is NaN (as it's a critical identifier for sorting)\n",
    "            df.dropna(subset=['index_id'], inplace=True)\n",
    "            \n",
    "            # Convert index_id to integer type after dropping NaNs\n",
    "            df['index_id'] = df['index_id'].astype(int)\n",
    "\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"Fișier procesat cu succes: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Eroare la procesarea fișierului {file_path}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"Nu există DataFrame-uri valide pentru a fi concatenate. Verificați fișierele de intrare.\")\n",
    "        return\n",
    "\n",
    "    concatenated_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Sort the final DataFrame by 'index_id'\n",
    "    concatenated_df.sort_values(by='index_id', inplace=True, ascending=True)\n",
    "\n",
    "    # Save the result\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    concatenated_df.to_csv(output_path, index=False)\n",
    "    print(f\"Datele concatenate și standardizate au fost salvate în: {output_path}\")\n",
    "\n",
    "# --- Instrucțiuni de utilizare ---\n",
    "# Asigurați-vă că fișierele CSV sunt plasate în același director cu acest script\n",
    "# sau specificați calea absolută către directorul care conține fișierele.\n",
    "\n",
    "# De exemplu, pentru a rula scriptul în directorul curent:\n",
    "current_directory = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\"\n",
    "concatenate_and_standardize_csv_files(current_directory)\n",
    "\n",
    "# Dacă fișierele se află într-un subdirector numit 'data' în cadrul directorului curent:\n",
    "# data_folder_path = os.path.join(current_directory, 'data')\n",
    "# concatenate_and_standardize_csv_files(data_folder_path)\n",
    "\n",
    "# Puteți specifica și o cale absolută direct:\n",
    "# specific_folder_path = \"/cale/catre/directorul/cu/fisiere\"\n",
    "# concatenate_and_standardize_csv_files(specific_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\building_places_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\building_places_batch_1_aligned.csv\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\creative_work_batch_1_aligned.csv\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\creative_work_batch_2_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\event_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\event_batch_1_aligned.csv\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_1_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_2_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_2_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_3_alignment.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_3_alignment.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_4_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\person_batch_4_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_1_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_2_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\place_batch_2_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_1_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_1_aligned.csv\n",
      "Avertisment: Antetul standard nu a fost găsit explicit în C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_2_aligned.csv. Se încearcă inferența coloanelor.\n",
      "Fișier procesat cu succes: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\uncertain_batch_2_aligned.csv\n",
      "Datele concatenate și standardizate au fost salvate în: C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\\entity_alignment.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np # Import numpy for inf values\n",
    "\n",
    "def concatenate_and_standardize_csv_files(folder_path, output_filename=\"entity_alignment.csv\"):\n",
    "    \"\"\"\n",
    "    Concatenates multiple CSV files from a specified folder, standardizes column names,\n",
    "    sorts by 'index_id', and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "        output_filename (str): The name of the output CSV file.\n",
    "    \"\"\"\n",
    "    standard_columns = ['index_id', 'candidate_id', 'confidence_score', 'need_judge']\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Get all CSV file paths in the specified folder\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.csv')]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"Nu s-au găsit fișiere CSV în directorul: {folder_path}. Asigurați-vă că fișierele sunt prezente.\")\n",
    "        return\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            # Read the file with no header initially to find the correct header row\n",
    "            df_raw = pd.read_csv(file_path, header=None, encoding='utf-8')\n",
    "\n",
    "            # Look for the header row containing all standard column names\n",
    "            header_row_index = -1\n",
    "            for i, row in df_raw.iterrows():\n",
    "                row_values = row.dropna().astype(str).tolist()\n",
    "                \n",
    "                # Check if all standard column names (or close variations) are present\n",
    "                # Use a more flexible check: any of the keywords in any of the row values\n",
    "                if all(any(col.lower() in val.lower() for val in row_values) for col in standard_columns):\n",
    "                    header_row_index = i\n",
    "                    break\n",
    "            \n",
    "            if header_row_index == -1:\n",
    "                print(f\"Avertisment: Antetul standard nu a fost găsit explicit în {file_path}. Se încearcă inferența coloanelor.\")\n",
    "                # Fallback: Try to infer based on typical data start\n",
    "                potential_data_start_row = -1\n",
    "                for i, row in df_raw.iterrows():\n",
    "                    first_val = row.dropna().iloc[0] if not row.dropna().empty else None\n",
    "                    if pd.notna(first_val) and (isinstance(first_val, (int, float)) or str(first_val).replace('.', '').isdigit() or str(first_val).lower() in ['true', 'false']):\n",
    "                        potential_data_start_row = i\n",
    "                        break\n",
    "                \n",
    "                if potential_data_start_row != -1:\n",
    "                    df = pd.read_csv(file_path, header=potential_data_start_row, encoding='utf-8')\n",
    "                    # Assume column order based on typical CSV structure if header not found\n",
    "                    if len(df.columns) >= len(standard_columns):\n",
    "                        df.rename(columns={df.columns[0]: 'index_id',\n",
    "                                           df.columns[1]: 'candidate_id',\n",
    "                                           df.columns[2]: 'confidence_score',\n",
    "                                           df.columns[3]: 'need_judge'}, inplace=True)\n",
    "                        df = df[standard_columns]\n",
    "                    else:\n",
    "                        print(f\"Se omite {file_path} din cauza numărului insuficient de coloane după inferență.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Se omite {file_path} deoarece nu a fost găsit un antet sau un început de date identificabil.\")\n",
    "                    continue\n",
    "            else:\n",
    "                # Read the file again with the identified header row\n",
    "                df = pd.read_csv(file_path, header=header_row_index, encoding='utf-8')\n",
    "\n",
    "                # Standardize column names based on content\n",
    "                col_mapping = {}\n",
    "                for std_col in standard_columns:\n",
    "                    # Find the first column that contains the standard column name (case-insensitive)\n",
    "                    matching_cols = [col for col in df.columns if std_col.lower() in str(col).lower()]\n",
    "                    if matching_cols and matching_cols[0] not in col_mapping:\n",
    "                        col_mapping[matching_cols[0]] = std_col\n",
    "                    else:\n",
    "                        print(f\"Avertisment: Coloana '{std_col}' nu a fost găsită sau este duplicată în {file_path}. S-ar putea să existe probleme de mapare.\")\n",
    "\n",
    "                # Apply mapping and select only standard columns\n",
    "                df.rename(columns=col_mapping, inplace=True)\n",
    "                df = df[[col for col in standard_columns if col in df.columns]]\n",
    "                \n",
    "                if len(df.columns) != len(standard_columns):\n",
    "                    print(f\"Avertisment: După standardizare, fișierul {file_path} nu conține toate coloanele așteptate: {standard_columns}. Coloane actuale: {list(df.columns)}\")\n",
    "\n",
    "            # Ensure all standard columns are present, fill missing with NaN\n",
    "            for col in standard_columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            df = df[standard_columns] # Reorder columns to be consistent\n",
    "\n",
    "            # --- Start improved data type conversion ---\n",
    "            # Convert to numeric, coercing errors and infinities to NaN\n",
    "            for col in ['index_id', 'candidate_id', 'confidence_score']:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    # Replace inf with NaN to ensure dropna catches them\n",
    "                    df[col] = df[col].replace([np.inf, -np.inf], pd.NA)\n",
    "\n",
    "            # Drop rows where index_id is NaN (as it's a critical identifier)\n",
    "            df.dropna(subset=['index_id'], inplace=True)\n",
    "            \n",
    "            # Convert index_id to integer type (should be safe now after dropping NaNs and infs)\n",
    "            df['index_id'] = df['index_id'].astype(int)\n",
    "\n",
    "            # For candidate_id, use Int64Dtype to allow for nullable integers (if candidate_id can be missing)\n",
    "            if 'candidate_id' in df.columns:\n",
    "                 df['candidate_id'] = df['candidate_id'].astype(pd.Int64Dtype())\n",
    "            \n",
    "            # Convert need_judge to boolean\n",
    "            if 'need_judge' in df.columns:\n",
    "                df['need_judge'] = df['need_judge'].astype(str).str.lower().isin(['true', '1', 'yes'])\n",
    "            # --- End improved data type conversion ---\n",
    "\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"Fișier procesat cu succes: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Eroare la procesarea fișierului {file_path}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"Nu există DataFrame-uri valide pentru a fi concatenate. Verificați fișierele de intrare.\")\n",
    "        return\n",
    "\n",
    "    concatenated_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Sort the final DataFrame by 'index_id'\n",
    "    concatenated_df.sort_values(by='index_id', inplace=True, ascending=True)\n",
    "\n",
    "    # Save the result\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    concatenated_df.to_csv(output_path, index=False)\n",
    "    print(f\"Datele concatenate și standardizate au fost salvate în: {output_path}\")\n",
    "\n",
    "# --- Instrucțiuni de utilizare ---\n",
    "# Asigurați-vă că fișierele CSV sunt plasate în același director cu acest script\n",
    "# sau specificați calea absolută către directorul care conține fișierele.\n",
    "\n",
    "# De exemplu, pentru a rula scriptul în directorul curent:\n",
    "current_directory = r\"C:\\Users\\DavidCaraman\\Desktop\\Licenta 1.0\\mvp-kg-alignment\\aligned_enitites\"\n",
    "concatenate_and_standardize_csv_files(current_directory)\n",
    "\n",
    "# Dacă fișierele se află într-un subdirector numit 'data' în cadrul directorului curent:\n",
    "# data_folder_path = os.path.join(current_directory, 'data')\n",
    "# concatenate_and_standardize_csv_files(data_folder_path)\n",
    "\n",
    "# Puteți specifica și o cale absolută direct:\n",
    "# specific_folder_path = \"/cale/catre/directorul/cu/fisiere\"\n",
    "# concatenate_and_standardize_csv_files(specific_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
